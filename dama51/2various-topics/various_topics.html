<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="generator" content="Pelican"/>
    <title>Various Topics from TM2.pdf Notes</title>
    <link rel="stylesheet" href="/dama51/theme/css/main.css"/>
    <script>
        MathJax = {
            tex: {
                inlineMath: [["$", "$"], ["\\(", "\\)"]],
                displayMath: [["$$", "$$"], ["\\[", "\\]"]],
                processEscapes: true
            },
            svg: {
                fontCache: "global"
            },
            options: {
                enableMenu: false  // Disable right-click MathJax menu
            }
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


        <meta name="description" content="Various Topics in machine learning and data analysis emphasizing dimensionality reduction, data visualization, hypothesis testing, and model evaluation" />
</head>

<style>
    :root {
        --background-image-uri: url("/dama51/images/background.png");
    }
</style>


<body id="index" class="home">
<header id="banner" class="body">
    <h1><a href="/dama51/">DAMA51</a>
    </h1>
    <nav>
        <ul>
            <li
><a href="/dama51/1introduction.html">1.Introduction</a></li>
            <li
 class="active"><a href="/dama51/2various-topics.html">2.Various Topics</a></li>
            <li
><a href="/dama51/3foundations.html">3.Foundations</a></li>
            <li
><a href="/dama51/4algorithms.html">4.Algorithms</a></li>
        </ul>
    </nav>
</header><!-- /#banner -->

<section id="content" class="body">
  <article>
      <header>
        <h1 class="entry-title">
          <a href="/dama51/2various-topics/various_topics.html" rel="bookmark"
             title="Permalink to Various Topics from TM2.pdf Notes">Various Topics from TM2.pdf Notes</a></h1>
      </header>

      <div class="entry-content">
        <p>The unit covers various topics in <strong>machine learning and data analysis</strong>, emphasizing <strong>dimensionality reduction, data visualization, hypothesis testing, and model evaluation</strong>. Below are key takeaways and what you need to understand:</p>
<h3><strong>1. High-Dimensional Data Visualization</strong></h3>
<ul>
<li><strong>Parallel Coordinates Plot</strong>: Represents multi-dimensional data by mapping multiple attributes onto parallel axes.  </li>
<li><strong>Radar and Star Plots</strong>: Useful for comparing a small number of attributes among different entities.  </li>
<li><strong>Sunburst Charts</strong>: Used for hierarchical data visualization.  </li>
<li><strong>3D Visualization</strong>: Used for understanding three-dimensional relationships, such as scatter plots in R.  </li>
</ul>
<hr>
<h3><strong>2. Dimensionality Reduction</strong></h3>
<ul>
<li><strong>Why Reduce Dimensionality?</strong>:<ul>
<li>High-dimensional data leads to computational inefficiencies.</li>
<li>Too many features can cause overfitting.</li>
</ul>
</li>
<li><strong>Principal Component Analysis (PCA)</strong>:<ul>
<li>Converts correlated variables into a set of uncorrelated principal components.</li>
<li>Helps in reducing dimensions while retaining most of the variance.</li>
<li>Eigenvalues and eigenvectors play a crucial role in feature transformation.</li>
<li>Standardization before applying PCA ensures fair weightage across features.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>3. Hypothesis Testing</strong></h3>
<ul>
<li><strong>Concepts Covered</strong>:<ul>
<li>Null hypothesis (\( H_0 \)) vs. Alternative hypothesis (\( H_a \))</li>
<li>One-sided vs. Two-sided tests.</li>
<li>p-value interpretation: A lower p-value (\(\leq\) 0.05) suggests strong evidence against \( H_0 \).</li>
</ul>
</li>
<li><strong>Type I and Type II Errors</strong>:<ul>
<li>Type I (False Positive): Rejecting \( H_0 \) when it is actually true.</li>
<li>Type II (False Negative): Failing to reject \( H_0 \) when it is false.</li>
</ul>
</li>
</ul>
<h3><strong>4. Model Evaluation &amp; Validation</strong></h3>
<ul>
<li><strong>Bias-Variance Tradeoff</strong>:<ul>
<li><strong>High bias</strong>: Model is too simple and underfits.</li>
<li><strong>High variance</strong>: Model is too complex and overfits.</li>
</ul>
</li>
<li><strong>Cross-Validation</strong>:<ul>
<li><strong>k-Fold Cross-Validation</strong>: Splitting data into multiple training and validation sets.</li>
<li><strong>Stratified Sampling</strong>: Ensuring class distributions remain balanced.</li>
</ul>
</li>
<li><strong>Overfitting &amp; Regularization</strong>:<ul>
<li>Overfitting occurs when a model performs well on training data but poorly on new data.</li>
<li><strong>Regularization techniques</strong> like L1 (Lasso) and L2 (Ridge) add penalties to model complexity.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>5. Feature Selection &amp; Data Cleaning</strong></h3>
<ul>
<li><strong>Feature Selection Methods</strong>:<ul>
<li><strong>Forward Selection</strong>: Starts with no features and adds them based on performance improvement.</li>
<li><strong>Backward Elimination</strong>: Starts with all features and removes the least important ones.</li>
<li><strong>Low Variance Filtering</strong>: Removes features with minimal variance.</li>
<li><strong>Correlation Filtering</strong>: Eliminates highly correlated features to reduce redundancy.</li>
</ul>
</li>
<li><strong>Data Normalization &amp; Standardization</strong>:<ul>
<li><strong>Min-Max Scaling</strong>: Rescales data between 0 and 1.</li>
<li><strong>Z-score Normalization</strong>: Transforms data to have mean = 0 and standard deviation = 1.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>6. Statistical &amp; Performance Metrics</strong></h3>
<ul>
<li><strong>Accuracy, Precision, Recall, F1-score</strong>:<ul>
<li><strong>Precision</strong>: Measures how many positive predictions were correct.</li>
<li><strong>Recall (Sensitivity)</strong>: Measures how many actual positives were detected.</li>
<li><strong>F1-score</strong>: Harmonic mean of precision and recall.</li>
</ul>
</li>
<li><strong>Receiver Operating Characteristic (ROC) Curve</strong>:<ul>
<li>Visualizes trade-offs between sensitivity and specificity.</li>
<li>Area Under Curve (AUC) quantifies classifier performance.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>7. Gradient Descent &amp; Model Optimization</strong></h3>
<ul>
<li><strong>Gradient Descent Algorithm</strong>:<ul>
<li>Used for optimizing model parameters by minimizing loss functions.</li>
<li><strong>Learning Rate</strong>:<ul>
<li>Too high: May not converge.</li>
<li>Too low: Slow convergence.</li>
</ul>
</li>
<li><strong>Variants</strong>: Batch, Stochastic (SGD), and Mini-Batch Gradient Descent.</li>
</ul>
</li>
<li><strong>Loss Functions</strong>:<ul>
<li>Mean Squared Error (MSE) for regression.</li>
<li>Cross-entropy loss for classification.</li>
</ul>
</li>
</ul>
<hr>
<h3><strong>What You Need to Know?</strong></h3>
<p>To effectively use this knowledge:<br>
- <strong>Understand Data Structures</strong>: Be comfortable with tabular datasets and different data types.<br>
- <strong>Learn Statistical Concepts</strong>: Have a grasp on probability distributions, hypothesis testing, and p-values.<br>
- <strong>Practice Visualization</strong>: Use Pythonâ€™s Matplotlib/Seaborn or Râ€™s ggplot2 for creating different plots.<br>
- <strong>Get Hands-On with PCA &amp; Feature Engineering</strong>: Apply dimensionality reduction to datasets and interpret results.<br>
- <strong>Train and Evaluate ML Models</strong>: Use <strong>train-test splitting, cross-validation, and regularization</strong> techniques.<br>
- <strong>Work with ML Algorithms</strong>: Get familiar with regression, classification, and optimization methods like gradient descent.  </p>
<p>Would you like help with implementing any of these concepts in <strong>Python or R</strong>? ðŸš€</p>
      </div><!-- /.entry-content -->
  </article>
</section>


</body>
</html>